# ============================================
# API KEYS
# ============================================
OPENAI_API_KEY=sk-your-api-key-here  # Get from https://platform.openai.com/api-keys

# ============================================
# LLM CONFIGURATION (for RAG generation)
# ============================================
LLM_PROVIDER=openai          # Options: "openai" or "vllm"
MODEL=gpt-4o-mini            # OpenAI model to use

# vLLM Configuration (Optional - only needed if LLM_PROVIDER=vllm)
# VLLM_BASE_URL=http://localhost:8000
# VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct
# VLLM_API_KEY=sk-local

# ============================================
# EMBEDDING CONFIGURATION (for document ingestion)
# ============================================
EMBEDDING_PROVIDER=openai    # Options: "local" or "openai"

# OpenAI Embeddings (when EMBEDDING_PROVIDER=openai)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Local Embeddings (when EMBEDDING_PROVIDER=local)
# LOCAL_EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
# LOCAL_MODELS_CACHE_DIR=./models

# ============================================
# CHUNKING CONFIGURATION
# ============================================
CHUNKING_METHOD=sentence-window     # Options: "token", "char", "semantic", or "sentence-window"
CHUNK_SIZE=450               # Number of tokens (for token method) or chars (for char method)
CHUNK_OVERLAP=90             # Overlap size (recommended: 20% of CHUNK_SIZE)

# ============================================
# SEMANTIC CHUNKING (LLM-powered) ðŸ†•
# ============================================
# Advanced chunking that guarantees single-topic chunks:
# - LLM lists ALL sections (not just patterns)
# - Each section = one chunk (never mixes topics)
# - Large sections split at paragraph breaks (not token boundaries)
SEMANTIC_CHUNKING_LLM_MODEL=gpt-4o-mini      # LLM for section detection
SEMANTIC_CHUNKING_ENABLE_LLM=true            # true = LLM mode (~$0.01/doc, best quality)
                                             # false = regex mode (free, instant, good quality)
SEMANTIC_CHUNKING_SAMPLE_SIZE=16000          # Chars to analyze for structure (larger = finds more sections)
                                             # Recommended: 12000-20000 for complex docs

# ============================================
# SENTENCE-WINDOW CHUNKING ðŸ†•
# ============================================
# Precision retrieval with expandable context:
# - Small chunks (50 tokens) for precise matching
# - Auto-expand to Â±2 sentences at retrieval time
# - Best for: Q&A, fact extraction, precise queries
SENTENCE_WINDOW_CHUNK_TOKENS=50          # Size of core chunks (smaller = more precise)
SENTENCE_WINDOW_SENTENCES=2              # Context sentences before/after (0-5 recommended)
