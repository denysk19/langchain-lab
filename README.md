# RAG Chatbot with Memory & Document Ingestion

Console-based RAG chatbot with persistent memory, modular architecture, and advanced PDF ingestion.

## Quick Start

```bash
# 1. Install dependencies
poetry install

# 2. Configure environment
cp .env_example .env
# Edit .env and add: OPENAI_API_KEY=sk-your-key-here

# 3. Add PDF documents
cp your-document.pdf docs/

# 4. Run chatbot
poetry run python scripts/chat.py --docs docs/
```

## Features

- **Persistent Conversations**: Thread-based memory stored in `.state/`
- **PDF Ingestion**: Dedicated ingestion module for document processing
- **In-Memory Vector Store**: Fast MemoryStore with multi-tenancy support
- **Smart Routing**: Only retrieves documents when needed
- **Multi-Provider**: Switch between OpenAI and vLLM
- **Source Citations**: Answers include document references
- **Evaluation Framework**: Comprehensive testing with golden datasets and metrics tracking

## Installation

### Prerequisites

- Python 3.11+
- Poetry (`curl -sSL https://install.python-poetry.org | python3 -`)
- OpenAI API key

### Setup

The project uses two git submodules (both in `src/`):
- `src/ingestion` - Document ingestion module
- `src/rag-module` - RAG workflow module

```bash
# Clone with submodules
git clone --recurse-submodules <repo-url>
cd langchain-lab

# If submodules not initialized
git submodule update --init --recursive

# Install everything (including submodules)
poetry install
```

### Configuration

Create `.env` file:

```bash
# Required
OPENAI_API_KEY=sk-your-key-here

# LLM Provider (choose one)
LLM_PROVIDER=openai              # Use OpenAI
MODEL=gpt-4o-mini

# Or use vLLM
# LLM_PROVIDER=vllm
# VLLM_BASE_URL=http://localhost:8000
# VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct
```

## Usage

### Basic Commands

```bash
# Run chatbot
poetry run python scripts/chat.py --docs docs/

# With verbose logging
poetry run python scripts/chat.py --docs docs/ --verbose

# Custom configuration
poetry run python scripts/chat.py \
  --docs docs/ \
  --k 10 \
  --chunk-chars 1200 \
  --chunk-overlap 200
```

### Using Poetry Shell

```bash
# Activate environment (no need to type 'poetry run' for each command)
poetry shell

python scripts/chat.py --docs docs/
python test_integration.py

# Exit when done
exit
```

### In-Chat Commands

- `/exit`, `/quit` - Save and exit
- `/reset` - Clear conversation history
- `/workflow` - Show RAG workflow diagram
- `/threads` - List all threads
- `/new [id]` - Create new thread
- `/info` - Show session info

## Command-Line Options

```bash
# Basic
--docs DIRECTORY        # Documents directory (default: "docs")
--thread THREAD_ID      # Thread ID for memory (default: "default")
--user USER_ID          # User ID (default: "default")
--k NUMBER              # Documents to retrieve (default: 8)

# Ingestion
--tenant-id ID          # Tenant ID (default: "default-tenant")
--owner-user-id ID      # Document owner (default: "admin")
--visibility SCOPE      # "org" or "private" (default: "org")
--chunk-size NUMBER     # Chunk size (uses config default if not set)
--chunk-overlap NUMBER  # Overlap (uses config default if not set)

# Memory
--max-messages NUMBER   # Max messages per thread (default: 100)
--max-threads NUMBER    # Max threads per user (default: 10)

# Logging
--verbose, -v           # Verbose logging
--log-level LEVEL       # DEBUG, INFO, WARNING, ERROR
```

## Evaluation Framework

The evaluation framework runs the RAG system through golden datasets and tracks comprehensive metrics for experiment comparison and analysis.

### Quick Start - Running Evaluations

```bash
# Basic evaluation on golden dataset
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset \
  --docs docs/

# Single file evaluation
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset/part-001.json \
  --docs docs/

# With custom experiment name
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset \
  --experiment-name "baseline_v1" \
  --k 5
```

### Evaluation Results

Results are saved in `experiments/<timestamp>__<name>/`:

```
experiments/
└── 20251013_143000__gpt-4o-mini/
    ├── results.csv      # Detailed metrics for each question
    └── metadata.txt     # Experiment configuration and stats
```

#### results.csv Columns

| Column | Description |
|--------|-------------|
| `question` | Question from golden dataset |
| `llm_answer` | Answer generated by RAG system |
| `gold_answer` | Expected answer (reference) |
| `context` | Ground truth context (reference) |
| `retrieved_context` | Context actually retrieved by RAG |
| `bucket` | Category (direct, logical, process) |
| `difficulty` | Difficulty level (easy, medium, hard) |
| `latency_seconds` | Time taken to generate answer |
| `num_chunks_retrieved` | Count of chunks retrieved |
| `needs_retrieval` | Whether retrieval was triggered |
| `token_count_estimate` | Estimated tokens used |
| `cost_estimate` | Estimated cost in USD |

#### metadata.txt Contents

- Experiment configuration (models, chunking, parameters)
- Dataset information
- Git commit hashes (for reproducibility)
- Summary statistics (latency, cost, success rate)

### Evaluation Options

```bash
# Dataset
--dataset PATH          # JSON file or folder with golden dataset

# Experiment
--experiment-name NAME  # Custom name (default: auto-generated)
--docs DIRECTORY        # Documents directory (default: "docs")

# Retrieval
--k NUMBER              # Documents to retrieve (default: 8)

# Chunking (optional overrides)
--chunk-size NUMBER     # Override chunk size
--chunk-overlap NUMBER  # Override chunk overlap

# Context
--tenant-id ID          # Tenant ID (default: "default-tenant")
--visibility SCOPE      # "org" or "private" (default: "org")
```

### Golden Dataset Format

JSON format with question entries:

```json
[
  {
    "bucket": "direct",
    "difficulty": "easy",
    "question": "How many days of annual leave do I get?",
    "context": "Schedule 1: Core and Other Leave...",
    "gold_answer": "26 working days per year."
  }
]
```

### Analyzing Results

**Using Python/Pandas:**

```python
import pandas as pd

# Load results
df = pd.read_csv('experiments/20251013_143000__gpt-4o-mini/results.csv')

# Summary statistics
print(f"Average latency: {df['latency_seconds'].mean():.2f}s")
print(f"Total cost: ${df['cost_estimate'].sum():.4f}")

# Group by difficulty
stats = df.groupby('difficulty').agg({
    'latency_seconds': 'mean',
    'cost_estimate': 'sum',
    'question': 'count'
})
print(stats)
```

**Using command-line:**

```bash
# View metadata
cat experiments/20251013_143000__gpt-4o-mini/metadata.txt

# Count questions by bucket
cut -d',' -f5 experiments/*/results.csv | sort | uniq -c

# Average latency
awk -F',' 'NR>1 {sum+=$7; count++} END {print sum/count}' experiments/*/results.csv
```

### Comparing Experiments

Compare different configurations to evaluate:
- Impact of different embedding models
- Effect of chunk size on retrieval quality
- LLM model performance differences
- Cost vs. quality tradeoffs

```python
import pandas as pd

# Compare two experiments
baseline = pd.read_csv('experiments/20251013_143000__baseline/results.csv')
improved = pd.read_csv('experiments/20251013_150000__improved/results.csv')

print(f"Baseline: {baseline['latency_seconds'].mean():.2f}s, ${baseline['cost_estimate'].sum():.4f}")
print(f"Improved: {improved['latency_seconds'].mean():.2f}s, ${improved['cost_estimate'].sum():.4f}")
```

### Best Practices

1. **Use descriptive experiment names** - Makes tracking easier
2. **Run baseline experiments first** - Establish performance benchmarks
3. **Test on small datasets** - Validate setup before full runs
4. **Track git commits** - Ensure reproducibility
5. **Monitor costs** - Especially with paid API endpoints
6. **Keep metadata.txt files** - Document configuration for later analysis

### Example Workflow

```bash
# 1. Baseline evaluation with default settings
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset \
  --experiment-name "baseline_default" \
  --docs docs/

# 2. Test with larger chunks
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset \
  --experiment-name "baseline_large_chunks" \
  --chunk-size 800 \
  --chunk-overlap 160 \
  --docs docs/

# 3. Test with more retrieval context
poetry run python scripts/evaluate.py \
  --dataset golden_dataset/bank_golden_dataset \
  --experiment-name "baseline_k10" \
  --k 10 \
  --docs docs/

# 4. Compare results
ls -lh experiments/
```

For detailed documentation, see [experiments/README.md](experiments/README.md).

## Architecture

### Structure

```
src/
├── ingestion/              # Ingestion submodule
│   └── src/ingestion/
│       ├── memory_store.py # In-memory vector store
│       ├── models.py       # DocumentCtx
│       └── ingest.py       # PDF processing
├── rag-module/             # RAG workflow submodule
│   └── src/rag_workflow/
│       ├── graph.py        # LangGraph workflow
│       ├── nodes.py        # Workflow nodes
│       └── prompts.yaml    # Configurable prompts
└── adapters/
    └── memory_retriever_adapter.py  # LangChain adapter

scripts/
├── chat.py                 # Main chat application
└── evaluate.py             # Evaluation framework

docs/                       # Place PDF documents here
golden_dataset/             # Golden datasets for evaluation
experiments/                # Evaluation results (timestamped)
```

### Workflow

1. **Query Classification** - Determines if retrieval is needed
2. **Query Rewriting** - Context-aware enhancement
3. **Document Retrieval** - MemoryStore vector search
4. **Answer Generation** - Context-based or direct response
5. **Citation** - Automatic source references

### Ingestion Pipeline

1. PDFs loaded from `--docs` directory
2. Processed by `ingest_pdf()` (extract text, metadata)
3. Chunked with configurable size/overlap
4. Embedded using OpenAI embeddings
5. Stored in MemoryStore with multi-tenancy context
6. Retrieved via LangChain adapter

## Development

### Project Structure

```
langchain-lab/
├── .env                    # Environment config (not committed)
├── .env_example            # Environment template
├── pyproject.toml          # Poetry dependencies
├── poetry.lock             # Locked versions
├── docs/                   # PDF documents
├── golden_dataset/         # Golden datasets for evaluation
├── experiments/            # Evaluation results (timestamped)
├── scripts/
│   ├── chat.py            # Main chat application
│   └── evaluate.py        # Evaluation framework
├── src/
│   ├── adapters/          # LangChain adapters
│   ├── ingestion/         # Submodule: document ingestion
│   └── rag-module/        # Submodule: RAG workflow
├── tests/
│   └── test_rag_smoke.py
└── test_integration.py    # Integration verification
```

### Testing

```bash
# Run integration test
poetry run python test_integration.py

# Run unit tests
poetry run pytest tests/

# Code quality
poetry run ruff check scripts/ src/
```

### Working with Submodules

```bash
# Update submodules to latest
git submodule update --remote

# Update specific submodule
cd src/ingestion
git pull origin main
cd ../..

# Changes are immediately available (develop mode)
poetry install  # Only if dependencies changed
```


## Examples

### Basic Chat Session

```bash
$ poetry run python scripts/chat.py --docs docs/

Loading 1 PDF files...
✅ Ingested bank_handbook.pdf: 102 chunks
Using provider: openai, model: gpt-4o-mini
✅ Ready! User: default, Thread: default

default> What is the refund policy?
bot> According to the handbook, customers have 30 days for full refunds. [1]
(time: 2.3s)

default> Are there exceptions?
bot> Yes, digital products are non-refundable. Defective items can be returned anytime. [1]
(time: 1.8s)
```

### Multi-Tenancy

```bash
poetry run python scripts/chat.py \
  --tenant-id acme-corp \
  --owner-user-id john-doe \
  --visibility org \
  --docs /path/to/company-docs/
```

### Development with Verbose Logging

```bash
poetry run python scripts/chat.py \
  --docs docs/ \
  --log-level DEBUG \
  --verbose
```

## License

MIT License - see LICENSE file for details.